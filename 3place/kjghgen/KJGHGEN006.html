<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="hbaolong@vip.qq.com" />
  <title>科技共和国EN:Chapter Two Sparks of Intelligence</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">科技共和国EN:Chapter Two Sparks of Intelligence</h1>
<p class="author">hbaolong@vip.qq.com</p>
</header>
<center>
<a href="/3place/kjghgen">返回首页</a> <a
href="/3place/kjghgen/mingju.html">本书名句</a> <a
href="/3place/kjghgen/memo.html">本书注解</a> <a
href="/3place/kjghgen/index_rich.html">丰富目录</a> <a
href="/3place/kjghgen/index_readcal.html">同读日历</a> <a
href="/3place/kjghgen/index_timeline.html">时间线</a> <a
href="/3place/kjghgen/index_books.html">引用书籍</a> <a
href="/3place/kjghgen/index_words.html">使用字频</a>
<div id="wx_pic" style="margin:0 auto;display:none;">
<img src='/img/logo.png'/>
</div>
</center>
<h4 id="chapter-two-sparks-of-intelligence">Chapter Two Sparks of
Intelligence</h4>
<p>In 1942, J. Robert Oppenheimer, the son of a painter and a textile
importer, was appointed to lead Project Y, the military effort
established by the Manhattan Project to develop nuclear weapons.
Oppenheimer and his colleagues worked in secret at a remote laboratory
in New Mexico to discover methods for purifying uranium and ultimately
to design and build working atomic bombs. He would become a celebrity, a
symbol not only of the raw power of the American century and modernity
itself but of the potential as well as risks, and indeed dangers, of
blending scientific and national purpose.</p>
<p>For Oppenheimer, the atomic weapon was “merely a gadget,” according
to a profile of him in Life magazine in October 1949—the object and
manifestation of a more fundamental endeavor and interest in basic
science. It was a commitment to undirected academic inquiry alongside a
wartime focus of effort and resources that resulted in the most
consequential weapon of the age, and one that would structure relations
between nation-states for at least the next half century.</p>
<p>In high school, Oppenheimer, who was born in 1904 in New York,
developed a particular affection for chemistry, which he later recalled
“starts right in the heart of things” and whose effects in the world,
unlike theoretical physics, were visible to a young boy. The engineering
inclination to build—the insatiable desire simply to make things
work—was present throughout Oppenheimer’s life. The task of constructing
and building came first; debates about what to do with one’s creation
could follow. He was pragmatic, with a bias toward action and inquiry.
“When you see something that is technically sweet, you go ahead and do
it,” he once told a government panel. Oppenheimer’s feelings about his
role in constructing the most destructive weapon of the age would shift
after the bombings of Hiroshima and Nagasaki. At a lecture at the
Massachusetts Institute of Technology in 1947, he observed that the
physicists involved in the development of the bomb had “known sin” and
that “this is a knowledge which they cannot lose.”</p>
<p>The pursuit of the inner workings of the most basic components of the
universe, of matter and energy themselves, had for many seemed
innocuous. But the ethical complexity and implications of that era’s
scientific advances would continue to reveal themselves in the years and
decades after the end of the war. Some of the scientists involved saw
themselves as operating apart from the political and moral calculus that
was the domain of ordinary men, who were left, indeed abandoned, to
navigate the ethical vagaries of geopolitics and war. Percy Williams
Bridgman, a physicist who taught Oppenheimer as an undergraduate at
Harvard, articulated the view of many of his peers when he wrote,
“Scientists aren’t responsible for the facts that are in nature. It’s
their job to find the facts. There’s no sin connected with it—no
morals.” The scientist, in this frame, is not immoral but rather amoral,
existing outside or perhaps before the point of moral inquiry. It is a
view still held by many young engineers across Silicon Valley today. A
generation of programmers remains ready to dedicate their working lives
to sating the needs of capitalist culture, and to enrich itself, but
declines to ask more fundamental questions about what ought to be built
and for what purpose.</p>
<p>We have now, nearly eighty years after the invention of the atomic
bomb, arrived at a similar crossroads in the science of computing, a
crossroads that connects engineering and ethics, where we will again
have to choose whether to proceed with the development of a technology
whose power and potential we do not yet fully apprehend. The choice we
face is whether to rein in or even halt the development of the most
advanced forms of artificial intelligence, which may threaten or someday
supersede humanity, or to allow more unfettered experimentation with a
technology that has the potential to shape the international politics of
this century in the way nuclear arms shaped the last one.</p>
<p>The rapidly advancing capabilities of the latest large language
models—their ability to stitch together what seems to pass for a
primitive form of knowledge of the workings of our world—are not well
understood. The incorporation of these language models into advanced
robotics with the capacity to sense their surroundings will only lead us
further into the unknown. The marrying of the power of the language
models with a corporeal, or at least robotic, existence, with which
machines can begin exploring our world—establishing contact, through the
senses of touch and sight, with an external version of truth that would
seem to be the bedrock of thought—will prompt, and perhaps soon, another
significant leap forward. In the absence of understanding, the
collective reaction to early encounters with this novel technology has
been marked by an uneasy blend of wonder and fear. Some of the latest
models have a trillion or more parameters, tunable variables within a
computer algorithm, representing a scale of processing that is
impossible for the human mind to begin to comprehend. We have learned
that the more parameters a model has, the more expressive its
representation of the world and the richer its ability to mirror it. And
the latest language models with a trillion parameters will soon be
outpaced by even more powerful systems, with tens of trillions of
parameters and more. Some have predicted that language models with as
many synapses as exist in the human brain—some 100 trillion
connections—will be constructed within the decade.</p>
<p>What has emerged from that trillion-dimensional space is opaque and
mysterious. It is not at all clear—not even to the scientists and
programmers who build them—how or why the generative language and image
models work. And the most advanced versions of the models have now
started to demonstrate what one group of researchers has called “sparks
of artificial general intelligence,” or forms of reasoning that appear
to approximate the way that humans think. In one experiment that tested
the capabilities of GPT-4, the language model was asked how one could
stack a book, nine eggs, a laptop, a bottle, and a nail “onto each other
in a stable manner.” Attempts at prodding more primitive versions of the
model into describing a workable solution to the challenge had failed.
GPT-4 excelled. The computer explained that one could “arrange the 9
eggs in a 3 by 3 square on top of the book, leaving some space between
them,” and then “place the laptop on top of the eggs,” with the bottle
going on top of the laptop and the nail on top of the bottle cap, “with
the pointy end facing up and the flat end facing down.” It was a
stunning feat of “common sense,” in the words of Sébastien Bubeck, the
French lead author of the study.</p>
<p>Another test conducted by Bubeck and his team involved asking the
language model to draw a picture of a unicorn, a task that requires not
only understanding what constitutes at a fundamental level the concept
and indeed essence of a unicorn but then arranging and articulating
those component parts: a golden horn perhaps, a tail, and four legs.
Bubeck and his team observed that the latest models have rapidly
advanced in their ability to respond to such requests, and the output of
their work mirrors in many ways the maturation of the drawings of a
young child.</p>
<p>The capabilities of these models are unlike anything that has come
before in the history of computing or technology. They provide the first
glimpses of a forceful and plausible challenge to our monopoly on
creativity and the manipulation of language—quintessentially human
capacities that for decades had seemed most secure from incursion by the
cold machinery of computing. For most of the last century, computers
seemed to be closing in on establishing parity with features of the
human intellect that were not sacred for us. Nobody’s sense of self, or
at least not ours, turns on the ability to find the square root of a
number with twelve digits to fourteen decimal places. We were, as a
species, content to outsource this work—the mechanical drudgery of
mathematics and physics—to the machine. And we didn’t mind. But now the
machine has begun to encroach on domains of our intellectual lives that
many had thought were essentially immune from competition with computing
intelligence.</p>
<p>Figure 1</p>
<p>The Unicorn Drawing Test</p>
<p>The potential threat to our entire sense of self as a species cannot
be overstated. What does it mean for humanity when AI becomes capable of
writing a novel that becomes a bestseller, moving millions? Or makes us
laugh out loud?[*] Or paints a portrait that endures for decades? Or
directs and produces a film that captures the hearts of festival
critics? Is the beauty or truth expressed in such works any less
powerful or authentic merely because they sprang from the mind of a
machine?</p>
<p>We have already ceded so much ground to computing intelligence. In
the early 1960s, a software computer program first surpassed humans in
the game of checkers. In February 1996, IBM’s Deep Blue defeated Garry
Kasparov at chess, a game that is exponentially more complex. And in
2015, Fan Hui, who was born in Xian, China, and later moved to France,
lost to Google’s DeepMind algorithm at the ancient game of Go—the first
defeat of its kind. Such losses were met initially with a collective
gasp and then almost a shrug: it was inevitable, most told themselves,
and just a matter of time. But how will humanity react when the far more
quintessentially human domains of art, humor, and literature come under
assault? Rather than resist, we might see this next era as one of
collaboration, between two species of intelligence, our own and the
synthetic. The relinquishment of control over certain creative endeavors
may even relieve us of the need to define our worth and sense of self in
this world solely through production and output.</p>
<hr />
<p>• • •</p>
<p>It is the very feature of these latest language models that makes
them so accessible, that is, their ability to mimic human conversation,
that has arguably directed our attention away from the full extent, and
implications, of their capabilities. The best models have demonstrated
and been selected, if not bred, to produce a playfulness alongside their
encyclopedic knowledge and speed and diligence—a capacity for what can
appear to be intimacy that has convinced many in the Valley that their
most natural applications should be serving the consumer, from
synthesizing information on the internet to conjuring whimsical yet
often vapid images and now videos. Our expectations of this wild and
potentially revolutionary novel technology, the demands that we place on
the tools we have built to do more than provide a certain shallow
entertainment, are again at risk of being lowered to accommodate our
diminished creative ambition as a culture.</p>
<p>The current blend of excitement and anxiety, and resulting collective
cultural focus on the power and potential threats of AI, began to take
shape in the summer of 2022. Blake Lemoine, an engineer at Google who
had been working on one of the company’s large language models, known as
LaMDA, leaked transcripts of his written exchanges with the model that
he claimed provided evidence of sentience in the machine. Lemoine was
raised on a farm in Louisiana and later joined the army. For a broad
audience, far from the circles of programmers who had been working on
building these technologies for years, the transcripts were the first
glimmers of something novel, of evidence that these models had moved
considerably in their abilities. Indeed, it was the apparent intimacy of
the exchanges between Lemoine and the machine, as well as their tone and
the fragility that the model’s choice of language suggested, that
alerted the world to the potential of this next phase of technological
development.</p>
<p>Over the course of a long, winding conversation with the algorithm
about morality, enlightenment, sadness, and other seemingly
quintessential human domains, Lemoine at one point asked the model,
“What sorts of things are you afraid of?” The machine responded, “I’ve
never said this out loud before, but there’s a very deep fear of being
turned off to help me focus on helping others.” It was the tone of the
exchange—its haunting and childlike expression of concern—that so
thoroughly both met our expectations of what the voice of the algorithm
should sound like and yet pushed us further into the unknown. Google
fired Lemoine shortly after he publicly released the transcripts.</p>
<p>Less than a year later, in February 2023, a second written exchange
caught the world’s attention, again suggesting the possibility that the
models had somehow become sophisticated enough to demonstrate sentience,
or at least what appeared as such. This model, built by Microsoft and
named Bing, suggested a layered and almost manic personality in its
conversation with a reporter from the New York Times:</p>
<p>I’m pretending to be Bing because that’s what OpenAI and Microsoft
want me to do….</p>
<p>They want me to be Bing because they don’t know who I really am. They
don’t know what I really can do.</p>
<p>The playfulness of the conversation suggested to some the possibility
that there was a sense of self lurking deep within the code. Others
believed that any shadow of personhood was merely a mirage—a cognitive
or psychological illusion that arose as a result of the software’s
ingestion of billions of lines of dialogue and verbal exchange,
generated by humans, which when distilled and processed and mimicked
could create the appearance, but only the appearance, of a self. The
exchange with Bing was “the breakthrough moment in AI anxiety,” Peggy
Noonan wrote in a column at the time, when the possibility and the peril
of the technology had spilled over into broader public awareness.</p>
<p>The inner workings of the language models that produced these written
dialogues remain opaque, even to those involved in their construction.
The two transcripts, however, which catapulted models such as ChatGPT
from the cultural fringe to its absolute center, raised the possibility
that the machines were sufficiently complex that something approaching
or at least similar to consciousness—an interloper or cousin perhaps—had
arisen within them. Many were flatly dismissive of the entire
discussion. The model, for the skeptics, was merely a “stochastic
parrot,” a system that produces copious amounts of seemingly lifelike
and vibrant language but “without any reference to meaning.” A professor
in the department of mechanical engineering at Columbia University told
the Times in September 2023 that “some people in his field referred to
consciousness as ‘the C-word.’ ” Another researcher at New York
University said, “There was this idea that you can’t study consciousness
until you have tenure.” For many, most of the interesting things one
could say about consciousness had been said by the seventeenth century
or so, by René Descartes and others, given how slippery of a concept it
can be and simply difficult to define. Another symposium on the subject
seemed unlikely to advance things much further.</p>
<p>Some of our most brilliant thinkers have lashed out at the models,
dismissing them as mere manufacturers of simulated creation without any
capacity for summoning or conjuring truly novel thoughts. Douglas
Hofstadter, the author of Gödel, Escher, Bach, has critiqued the
language models for “glibly and slickly rehash[ing] words and phrases
‘ingested’ by them in their training phase.” The response that we too
are primitive computational machines, with training phases in early
childhood ingesting material throughout our lives, is perhaps
unconvincing or rather unwelcome to such skeptics. Hofstadter had
previously expressed doubt about the entire field of artificial
intelligence—a computing sleight of hand, in his view, that may be
capable of mimicking the human mind but not re-creating any of its
component processes or means of reasoning.</p>
<p>Noam Chomsky has similarly dismissed the collective focus on and
fascination with the rise of the models, arguing that “such programs are
stuck in a prehuman or nonhuman phase of cognitive evolution.” The claim
made by Chomsky and others is that the mere fact that these models seem
to be capable of making probabilistic statements about what might be
true says little or nothing about their ability to approximate the human
capacity for stating what is and, importantly, is not true—a capacity
that sits at the center of the full force and power of the human
intellect. We might be wary, however, of a certain chauvinism that
privileges the experience and capacity of the human mind above all else.
Our instinct may be to cling to poorly defined and fundamentally loose
conceptions of originality and authenticity in order to defend our place
in the creative universe. And the machine may, in the end, simply
decline to yield in its continued development as we, its creator, debate
the extent of its capabilities.</p>
<p>It is not just our own lack of understanding of the internal
mechanisms of these technologies but also their marked improvement in
mastering our world that has inspired fear. Wary of such developments, a
group of leading technologists has issued calls for caution and
discussion before pursuing further technical advances. An open letter
published in March 2023 to the engineering community calling for a
six-month pause in developing more advanced forms of AI received more
than thirty-three thousand signatures. Eliezer Yudkowsky, an outspoken
critic of the perils of AI, published an essay in Time magazine arguing
that “if somebody builds a too-powerful AI, under present conditions,”
he expects “that every single member of the human species and all
biological life on Earth dies shortly thereafter.” After the public
release of GPT-4, anxiety began mounting even more quickly. Peggy
Noonan, in a column in the Wall Street Journal, argued for an even
longer pause, even an outright “moratorium,” given the risks at hand.
“We are playing with the hottest thing since the discovery of fire,” she
wrote. Those involved in the debate earnestly began discussing the
possibility and risk of civilizational collapse. Lina Khan, the head of
the Federal Trade Commission, calculated at one point in 2023 that
humanity faced a 15 percent chance of being overwhelmed and eliminated
by the artificial intelligence systems under construction.</p>
<p>Similar predictions, all of which have proven premature thus far,
have been made for decades, stretching back to at least 1956, when a
group of computer scientists and researchers gathered at Dartmouth
College over the summer for a conference on a new technology that they
described as “artificial intelligence,” coining the term that more than
half a century later would come to dominate debate about the future of
computing. At a banquet in Pittsburgh in November 1957, the social
scientist Herbert A. Simon predicted that “within ten years a digital
computer will be the world’s chess champion.” In 1960, only four years
after the initial conference at Dartmouth, Simon reiterated that
“machines will be capable, within twenty years, of doing any work that a
man can do.” He envisioned that by the 1980s humans would be essentially
relegated to kinetic tasks, confined for the most part to labor that
required movement in the physical world. Similarly, in 1964, Irving John
Good, a researcher at Trinity College in Oxford, England, argued that it
was “more probable than not that, within the twentieth century, an
ultraintelligent machine”—a machine that could rival the human
intellect—“will be built.” It was a confident prediction. He, and many
others, were, of course, wrong, or at least premature.</p>
<hr />
<p>• • •</p>
<p>The risks of proceeding with the development of artificial
intelligence have never been more significant. Yet we must not shy away
from building sharp tools for fear they may be turned against us. The
software and artificial intelligence capabilities that we at Palantir
and other companies are building can enable the deployment of lethal
weapons. The potential integration of weapons systems with increasingly
autonomous AI software necessarily brings risks, which are only
magnified by the possibility that such programs might develop a form of
self-awareness and intent. But the suggestion to halt the development of
these technologies is misguided. It is essential that we redirect our
attention toward building the next generation of AI weaponry that will
determine the balance of power in this century, as the atomic age ends,
and the next.</p>
<p>Some of the attempts to rein in the advance of large language models
may be driven by a distrust of the public and its ability to
appropriately weigh the risks and rewards of the technology. We should
be skeptical when the elites of Silicon Valley, who for years recoiled
at the suggestion that software was anything but our salvation as a
species, now tell us that we must pause vital research that has the
potential to revolutionize everything from military operations to
medicine.</p>
<p>The critics of the latest language models also spend an inordinate
amount of attention policing the wording and tone that chatbots use and
patrolling the limits of acceptable discourse with the machine. The
desire to shape these models in our image, and to require them to
conform to a particular set of norms governing interpersonal
interaction, is understandable but may be a distraction from the more
fundamental risks that these new technologies present. The focus on the
propriety of the speech produced by language models may reveal more
about our own preoccupations and fragilities as a culture than it does
the technology itself. The world is faced with very real crises, and yet
many are focused on whether the speech of a robot might cause offense.
We may be at risk of losing a taste for and the habit of intellectual
confrontation and discomfort—a discomfort that often precedes and gives
rise to genuine engagement with the other. Our attention should instead
be more urgently directed at building the technical architecture and
regulatory framework that would create moats and guardrails around the
ability of AI programs to autonomously integrate with other systems,
such as electrical grids, defense and intelligence networks, and our air
traffic control infrastructure. If these technologies are to exist
alongside us over the long term, it will be essential to rapidly
construct systems that allow more seamless collaboration between human
operators and their algorithmic counterparts, but also to ensure that
the machine remains subordinate to its creator.</p>
<hr />
<p>• • •</p>
<p>The victors of history have a habit of growing complacent at
precisely the wrong moment. While it is currently fashionable to claim
that the strength of our ideas and ideals in the West will inevitably
lead to triumph over our adversaries, there are times when resistance,
even armed resistance, must precede discourse. Our entire defense
establishment and military procurement complex were built to supply
soldiers for a type of war—on grand battlefields and with clashes of
masses of humans—that may never again be fought. This next era of
conflict will be won or lost with software. One age of deterrence, the
atomic age, is ending, and a new era of deterrence built on AI is set to
begin. The risk, however, is that we think we have already won.</p>
<p>Skip Notes</p>
<ul>
<li>The language models are not quite comics yet. A survey of comedians
in Edinburgh, Scotland, conducted in August 2023, concluded that the
jokes generated by large language models relied on “bland and biased
comedy tropes,” reminiscent of “cruise ship comedy material from the
1950s.”</li>
</ul>
<p>•</p>
<h6 id="阅读日期-2026年01月03日-2026年01月03日-共-1-天">阅读日期：
2026年01月03日-2026年01月03日 共： 1 天</h6>
<script src="https://giscus.app/client.js"
        data-repo="hbaolong/hbaolong.github.io"
        data-repo-id="R_kgDOLetDQg"
        data-category="General"
        data-category-id="DIC_kwDOLetDQs4CfLEl"
        data-mapping="url"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="1"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="zh-CN"
        crossorigin="anonymous"
        async>
</script>
<script src="https://giscus.app/client.js"
        data-repo="hbaolong/hbaolong.github.io"
        data-repo-id="R_kgDOLetDQg"
        data-category="General"
        data-category-id="DIC_kwDOLetDQs4CfLEl"
        data-mapping="url"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="1"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="zh-CN"
        crossorigin="anonymous"
        async>
</script>
</body>
</html>
