<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="hbaolong@vip.qq.com" />
  <title>统计反思EN:4.1. Why normal distributions are normal</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">统计反思EN:4.1. Why normal distributions are
normal</h1>
<p class="author">hbaolong@vip.qq.com</p>
</header>
<center>
<a href="/3place/tjfsen">返回首页</a> <a
href="/3place/tjfsen/mingju.html">本书名句</a> <a
href="/3place/tjfsen/memo.html">本书注解</a> <a
href="/3place/tjfsen/index_rich.html">丰富目录</a> <a
href="/3place/tjfsen/index_readcal.html">同读日历</a> <a
href="/3place/tjfsen/index_timeline.html">时间线</a> <a
href="/3place/tjfsen/index_books.html">引用书籍</a> <a
href="/3place/tjfsen/index_words.html">使用字频</a>
<div id="wx_pic" style="margin:0 auto;display:none;">
<img src='/img/logo.png'/>
</div>
</center>
<h4 id="why-normal-distributions-are-normal">4.1. Why normal
distributions are normal</h4>
<p>Suppose you and a thousand of your closest friends line up on the
halfway line of a soccer field (football pitch). Each of you has a coin
in your hand. At the sound of the whistle, you begin flipping the coins.
Each time a coin comes up heads, that person moves one step towards the
left-hand goal. Each time a coin comes up tails, that person moves one
step towards the right-hand goal. Each person flips the coin 16 times,
follows the implied moves, and then stands still. Now we measure the
distance of each person from the halfway line. Can you predict what
proportion of the thousand people who are standing on the halfway line?
How about the proportion 5 yards left of the line?</p>
<p>It’s hard to say where any individual person will end up, but you can
say with great con- fidence what the collection of positions will be.
The distances will be distributed in approxi- mately normal, or
Gaussian, fashion. This is true even though the underlying distribution
is binomial. It does this because there are so many more possible ways
to realize a sequence of left-right steps that sums to zero. There are
slightly fewer ways to realize a sequence that ends up one step left or
right of zero, and so on, with the number of possible sequences
declining in the characteristic bell curve of the normal
distribution.</p>
<p>4.1.1. Normal by addition. Let’s see this result, by simulating this
experiment in R. To show that there’s nothing special about the
underlying coin flip, assume instead that each step is different from
all the others, a random distance between zero and one yard. Thus a coin
is flipped, a distance between zero and one yard is taken in the
indicated direction, and the process repeats. To simulate this, we
generate for each person a list of 16 random numbers between -1 and 1.
These are the individual steps. Then we add these steps together to get
the position after 16 steps. Then we need to replicate this procedure
1000 times. This is the sort of task that would be harrowing in a
point-and-click interface, but it is made trivial by the command line.
Here’s a single line to do the whole thing:</p>
<p>Figure 4.2. Random walks on the soccer field converge to a normal
dis- tribution. The more steps are taken, the closer the match between
the real empirical distribution of positions and the ideal normal
distribution, super- imposed in the last plot in the bottom panel.</p>
<p>You can plot the distribution of final positions in a number of
different ways, including hist(pos) and plot(density(pos)). In Figure
4.2, I show the result of these random walks and how their distribution
evolves as the number of steps increases. The top panel plots 100
different, independent random walks, with one highlighted in black. The
vertical dashes indicate the locations corresponding to the distribution
plots underneath, measured after 4, 8, and 16 steps. Although the
distribution of positions starts off seemingly idiosyn- cratic, after 16
steps, it has already taken on a familiar outline. The familiar “bell”
curve of the Gaussian distribution is emerging from the randomness. Go
ahead and experiment with even larger numbers of steps to verify for
yourself that the distribution of positions is stabi- lizing on the
Gaussian. You can square the step sizes and transform them in a number
of arbitrary ways, without changing the result: Normality emerges. Where
does it come from?</p>
<p>Any process that adds together random values from the same
distribution converges to a normal. But it’s not easy to grasp why
addition should result in a bell curve of sums.65 Here’s a conceptual
way to think of the process. Whatever the average value of the source
distribution, each sample from it can be thought of as a fluctuation
from that average value. When we begin to add these fluctuations
together, they also begin to cancel one another out. A large positive
fluctuation will cancel a large negative one. The more terms in the sum,
the more chances for each fluctuation to be canceled by another, or by a
series of smaller ones in the opposite direction. So eventually the most
likely sum, in the sense that there are the most ways to realize it,
will be a sum in which every fluctuation is canceled by another, a sum
of zero (relative to the mean).66</p>
<p>R code 4.2</p>
<p>R code 4.3</p>
<p>R code 4.4</p>
<p>It doesn’t matter what shape the underlying distribution possesses.
It could be uniform, like in our example above, or it could be (nearly)
anything else.67 Depending upon the un- derlying distribution, the
convergence might be slow, but it will be inevitable. Often, as in this
example, convergence is rapid.</p>
<p>4.1.2. Normal by multiplication. Here’s another way to get a normal
distribution. Suppose the growth rate of an organism is influenced by a
dozen loci, each with several alleles that code for more growth. Suppose
also that all of these loci interact with one another, such that each
increase growth by a percentage. This means that their effects multiply,
rather than add. For example, we can sample a random growth rate for
this example with this line of code:</p>
<p>This code just samples 12 random numbers between 1.0 and 1.1, each
representing a pro- portional increase in growth. Thus 1.0 means no
additional growth and 1.1 means a 10% increase. The product of all 12 is
computed and returned as output. Now what distribution do you think
these random products will take? Let’s generate 10,000 of them and
see:</p>
<p>The reader should execute this code in R and see that the
distribution is approximately nor- mal again. I said normal
distributions arise from summing random fluctuations, which is true. But
the effect at each locus was multiplied by the effects at all the
others, not added. So what’s going on here?</p>
<p>We again get convergence towards a normal distribution, because the
effect at each lo- cus is quite small. Multiplying small numbers is
approximately the same as addition. For example, if there are two loci
with alleles increasing growth by 10% each, the product is:</p>
<p>1.1 × 1.1 = 1.21</p>
<p>We could also approximate this product by just adding the increases,
and be off by only 0.01:</p>
<p>1.1 × 1.1 = (1 + 0. 1)(1 + 0. 1) = 1 + 0.2 + 0.01 ≈ 1.2</p>
<p>The smaller the effect of each locus, the better this additive
approximation will be. In this way, small effects that multiply together
are approximately additive, and so they also tend to stabilize on
Gaussian distributions. Verify this for yourself by comparing:</p>
<p>The interacting growth deviations, as long as they are sufficiently
small, converge to a Gauss- ian distribution. In this way, the range of
causal forces that tend towards Gaussian distribu- tions extends well
beyond purely additive interactions.</p>
<p>4.1.3. Normal by log-multiplication. But wait, there’s more. Large
deviates that are multi- plied together do not produce Gaussian
distributions, but they do tend to produce Gaussian distributions on the
log scale. For example:</p>
<p>Yet another Gaussian distribution. We get the Gaussian distribution
back, because adding logs is equivalent to multiplying the original
numbers. So even multiplicative interactions of large deviations can
produce Gaussian distributions, once we measure the outcomes on the log
scale. Since measurement scales are arbitrary, there’s nothing
suspicious about this transformation. After all, it’s natural to measure
sound and earthquakes and even informa- tion (7) on a log scale.</p>
<p>4.1.4. Using Gaussian distributions. We’re going to spend the rest
ofthis chapter using the Gaussian distribution as a skeleton for our
hypotheses, building up models of measurements as aggregations of normal
distributions. The justifications for using the Gaussian distribution
fall into two broad categories: (1) ontological and (2)
epistemological.</p>
<p>By the ontological justification, the world is full of Gaussian
distributions, approximately. We’re never going to experience a perfect
Gaussian distribution. But it is a widespread pat- tern, appearing again
and again at different scales and in different domains. Measurement
errors, variations in growth, and the velocities of molecules all tend
towards Gaussian distri- butions. These processes do this because at
their heart, these processes add together fluctu- ations. And repeatedly
adding finite fluctuations results in a distribution of sums that have
shed all information about the underlying process, aside from mean and
spread.</p>
<p>One consequence of this is that statistical models based on Gaussian
distributions can- not reliably identify micro-process. This recalls the
modeling philosophy from 1 (page 6). But it also means that these models
can do useful work, even when they cannot identify process. If we had to
know the development biology of height before we could build a
statistical model of height, human biology would be sunk.</p>
<p>There are many other patterns in nature, so make no mistake in
assuming that the Gauss- ian pattern is universal. In later chapters,
we’ll see how other useful and common patterns, like the exponential and
gamma and Poisson, also arise from natural processes. The Gauss- ian is
a member of a family of fundamental natural distributions known as the
exponential family. All of the members of this family are important for
working science, because they populate our world.</p>
<p>But the natural occurrence of the Gaussian distribution is only one
reason to build mod- els around it. By the epistemological
justification, the Gaussian represents a particular state of ignorance.
When all we know or are willing to say about a distribution of measures
(mea- sures are continuous values on the real number line) is their mean
and variance, then the Gaussian distribution arises as the most
consistent with our assumptions.</p>
<p>That is to say that the Gaussian distribution is the most natural
expression of our state of ignorance, because if all we are willing to
assume is that a measure has finite variance, the Gaussian distribution
is the shape that can be realized in the largest number of ways and does
not introduce any new assumptions. It is the least surprising and least
informative assumption to make. In this way, the Gaussian is the
distribution most consistent with our assumptions. Or rather, it is the
most consistent with our golem’s assumptions. If you don’t think the
distribution should be Gaussian, then that implies that you know
something else that you should tell your golem about, something that
would improve inference.</p>
<p>This epistemological justification is premised on information theory
and maximum entropy. We’ll dwell on information theory in 7 and maximum
entropy in Chap- ter 10. Then in later chapters, other common and useful
distributions will be used to build generalized linear models (GLMs).
When these other distributions are introduced, you’ll learn the
constraints that make them the uniquely most appropriate
distributions.</p>
<p>For now, let’s take the ontological and epistemological
justifications of just the Gaussian distribution as reasons to start
building models of measures around it. Throughout all of this modeling,
keep in mind that using a model is not equivalent to swearing an oath to
it. The golem is your servant, not the other way around.</p>
<p>Rethinking: Heavy tails. The Gaussian distribution is common in
nature and has some nice proper- ties. But there are some risks in using
it as a default data model. The extreme ends ofa distribution are known
as its tails. And the Gaussian distribution has some very thin
tails—there is very little prob- ability in them. Instead most of the
mass in the Gaussian lies within one standard deviation of the mean.
Many natural (and unnatural) processes have much heavier tails. These
processes have much higher probabilities of producing extreme events. A
real and important example is financial time series—the ups and downs of
a stock market can look Gaussian in the short term, but over medium and
long periods, extreme shocks make the Gaussian model (and anyone who
uses it) look foolish.68 Historical time series may behave similarly,
and any inference for example of trends in warfare is prone to
heavy-tailed surprises.69 We’ll consider alternatives to the Gaussian
later.</p>
<p>Overthinking: Gaussian distribution. You don’t have to memorize the
Gaussian probability distri- bution. You’re computer already knows it.
But some knowledge of its form can help demystify it. The probability
density (see below) of some value y, given a Gaussian (normal)
distribution with mean µ and standard deviation σ, is:</p>
<p>This looks monstrous. The important bit is just the (y - µ)2 bit.
This is the part that gives the normal distribution its fundamental
quadratic shape. Once you exponentiate the quadratic shape, you get the
classic bell curve. The rest ofit just scales and standardizes the
distribution.</p>
<p>The Gaussian is a continuous distribution, unlike the discrete
distributions of earlier chapters. Probability distributions with only
discrete outcomes, like the binomial, are called probability mass
functions and denoted Pr. Continuous ones like the Gaussian are called
probability density functions, denoted with p or just plain old f,
depending upon author and tradition. For mathematical reasons,
probability densities can be greater than 1. Try dnorm(0,0,0.1), for
example, which is the way to make R calculate p(0|0, 0. 1). The answer,
about 4, is no mistake. Probability density is the rate of change in
cumulative probability. So where cumulative probability is increasing
rapidly, density can easily exceed 1. But if we calculate the area under
the density function, it will never exceed 1. Such areas are also called
probability mass. You can usually ignore these density/mass details
while doing computational work. But it’s good to be aware of the
distinction. Sometimes the difference matters.</p>
<p>The Gaussian distribution is routinely seen without σ but with
another parameter, τ . The param- eter τ in this context is usually
called precision and defined as τ = 1/σ2 . When σ is large, τ is small.
This change of parameters gives us the equivalent formula (just
substitute σ = 1/√τ ):</p>
<p>This form is common in Bayesian data analysis, and Bayesian model
fitting software, such as BUGS or JAGS, sometimes requires using τ
rather than σ .</p>
<h6 id="阅读日期-2025年12月17日-2025年12月17日-共-1-天">阅读日期：
2025年12月17日-2025年12月17日 共： 1 天</h6>
<script src="https://giscus.app/client.js"
        data-repo="hbaolong/hbaolong.github.io"
        data-repo-id="R_kgDOLetDQg"
        data-category="General"
        data-category-id="DIC_kwDOLetDQs4CfLEl"
        data-mapping="url"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="1"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="zh-CN"
        crossorigin="anonymous"
        async>
</script>
</body>
</html>
