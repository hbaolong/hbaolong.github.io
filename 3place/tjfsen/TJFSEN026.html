<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="hbaolong@vip.qq.com" />
  <title>统计反思EN:4.3. Gaussian model of height</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">统计反思EN:4.3. Gaussian model of height</h1>
<p class="author">hbaolong@vip.qq.com</p>
</header>
<center>
<a href="/3place/tjfsen">返回首页</a> <a
href="/3place/tjfsen/mingju.html">本书名句</a> <a
href="/3place/tjfsen/memo.html">本书注解</a> <a
href="/3place/tjfsen/index_rich.html">丰富目录</a> <a
href="/3place/tjfsen/index_readcal.html">同读日历</a> <a
href="/3place/tjfsen/index_timeline.html">时间线</a> <a
href="/3place/tjfsen/index_books.html">引用书籍</a> <a
href="/3place/tjfsen/index_words.html">使用字频</a>
<div id="wx_pic" style="margin:0 auto;display:none;">
<img src='/img/logo.png'/>
</div>
</center>
<h4 id="gaussian-model-of-height">4.3. Gaussian model of height</h4>
<p>Let’s build a linear regression model now. Well, it’ll be a
“regression” once we have a predictor variable in it. For now, we’ll get
the scaffold in place and construct the predictor variable in the next
section. For the moment, we want a single measurement variable to model
as a Gaussian distribution. There will be two parameters describing the
distribution’s shape, the mean μ and the standard deviation σ . Bayesian
updating will allow us to consider every possible combination of values
for μ and σ and to score each combination by its relative</p>
<p>plausibility, in light of the data. These relative plausibilities are
the posterior probabilities of each combination of values μ, σ .</p>
<p>Another way to say the above is this. There are an infinite number of
possible Gaussian distributions. Some have small means. Others have
large means. Some are wide, with a large σ . Others are narrow. We want
our Bayesian machine to consider every possible distribution, each
defined by a combination of μ and σ, and rank them by posterior
plausibility. Posterior plausibility provides a measure of the logical
compatibility of each possible distribution with the data and model.</p>
<p>In practice we’ll use approximations to the formal analysis. So we
won’t really consider every possible value of μ and σ . But that won’t
cost us anything in most cases. Instead the thing to worry about is
keeping in mind that the “estimate” here will be the entire posterior
distribution, not any point within it. And as a result, the posterior
distribution will be a distribution of Gaussian distributions. Yes, a
distribution of distributions. If that doesn’t make sense yet, then that
just means you are being honest with yourself. Hold on, work hard, and
it will make plenty of sense before long.</p>
<p>4.3.1. The data. The data contained in data(Howell1) are partial
census data for the Dobe area !Kung San, compiled from interviews
conducted by Nancy Howell in the late 1960s.70 For the
non-anthropologists reading along, the !Kung San are the most famous
foraging population of the twentieth century, largely because of
detailed quantitative studies by people like Howell. Load the data and
place them into a convenient object with:</p>
<p>What you have now is a data frame named simply d. I use the name d
over and over again in this book to refer to the data frame we are
working with at the moment. I keep its name short to save you typing. A
data frame is a special kind of object in R. It is a table with named
columns, corresponding to variables, and numbered rows, corresponding to
individ- ual cases. In this example, the cases are individuals. Inspect
the structure of the data frame, the same way you can inspect the
structure of any symbol in R:</p>
<p>‘data .frame’: 544 obs . of 4 variables: $ height: num 152 140 137
157 145 . . .</p>
<p>$ weight: num 47 .8 36 .5 31 .9 53 41 .3 . . .</p>
<p>$ age : num 63 63 65 41 51 35 32 27 19 54 . . .</p>
<p>$ male : int 1 0 0 1 0 1 0 1 0 1 . . .</p>
<p>We can also use rethinking’s precis summary function, which we’ll
also use to summarize posterior distributions later on:</p>
<p>‘data .frame’: 544 obs . of 4 variables:</p>
<p>mean sd 5 .5% 94 .5% histogram height 138 .26 27 .60 81 .11 165 .74
一一一一一一一▂一▇▇▅一weight 35 .61 14 .72 9 .36 54 .50
一▂▃▂▂▂▂▅▇▇▃▂一</p>
<p>R code 4.7</p>
<p>R code 4.8</p>
<p>R code 4.9</p>
<p>age 29.34 20.75 1.00 66.13 ▇▅▅▃▅▂▂一一male 0.47 0.50 0.00 1.00
▇一一一一一一一一▇</p>
<p>If you cannot see the histograms on your system, use instead
precis(d,hist=FALSE). This data frame contains four columns. Each column
has 544 entries, so there are 544 individuals in these data. Each
individual has a recorded height (centimeters), weight (kilograms), age
(years), and “maleness” (0 indicating female and 1 indicating male).</p>
<p>We’re going to work with just the height column, for the moment. The
column con- taining the heights is really just a regular old R vector,
the kind of list we have been working with in many of the code examples.
You can access this vector by using its name:</p>
<p>Read the symbol $ as extract, as in extract the column named
heightfrom the data frame d.</p>
<p>All we want for now are heights of adults in the sample. The reason
to filter out non- adults for now is that height is strongly correlated
with age, before adulthood. Later in the chapter, I’ll ask you to tackle
the age problem. But for now, better to postpone it. You can filter the
data frame down to individuals of age 18 or greater with:</p>
<p>We’ll be working with the data frame d2 now. It should have 352 rows
(individuals) in it.</p>
<p>Overthinking: Data frames and indexes. The square bracket notation
used in the code above is index notation. It is very powerful, but also
quite compact and confusing. The data frame d is a matrix, a rectangular
grid of values. You can access any value in the matrix with d[row,col],
replacing row and col with row and column numbers. If row or col are
lists of numbers, then you get more than one row or column. If you leave
the spot for row or col blank, then you get all of whatever you leave
blank. For example, d[ 3 , ] gives all columns at row 3. Typing d[,]
just gives you the entire matrix, because it returns all rows and all
columns.</p>
<p>So what d[ d &gt;= 18 , ] does is give you all of the rows in which d
is greater-than- or-equal-to 18. It also gives you all of the columns,
because the spot after the comma is blank. The result is stored in d2,
the new data frame containing only adults. With a little practice, you
can use this square bracket index notion to perform custom searches of
your data, much like performing a database query.</p>
<p>It might seem like this whole data frame thing is unnecessary. If
we’re working with only one column here, why bother with this d thing at
all? You don’t have to use a data frame, as you can just pass raw
vectors to every command we’ll use in this book. But keeping related
variables in the same data frame is a convenience. Once we have more
than one variable, and we wish to model one as a function of the others,
you’ll better see the value of the data frame. You won’t have to wait
long. More technically, a data frame is a special kind of list in R. So
you access the individual variables with the usual list “double bracket”
notation, like d[[1]] for the first variable or d[[‘x’]] for the
variable named x. Unlike regular lists, however, data frames force all
variables to have the same length. That isn’t always a good thing. In
the second half of the book, we’ll start using ordinary list collections
instead of data frames.</p>
<p>4.3.2. The model. Our goal is to model these values using a Gaussian
distribution. First, go ahead and plot the distribution of heights, with
dens(d2). These data look rather Gaussian in shape, as is typical of
height data. This may be because height is a sum of many small growth
factors. As you saw at the start of the chapter, a distribution of sums
tends</p>
<p>o converge to a Gaussian distribution. Whatever the reason, adult
heights from a single population are nearly always approximately
normal.</p>
<p>So it’s reasonable for the moment to adopt the stance that the model
should use a Gauss- ian distribution for the probability distribution of
the data. But be careful about choosing the Gaussian distribution only
when the plotted outcome variable looks Gaussian to you. Gawking at the
raw data, to try to decide how to model them, is usually not a good
idea. The data could be a mixture of different Gaussian distributions,
for example, and in that case you won’t be able to detect the underlying
normality just by eyeballing the outcome distribu- tion. Furthermore, as
mentioned earlier in this chapter, the empirical distribution needn’t be
actually Gaussian in order to justify using a Gaussian probability
distribution.</p>
<p>So which Gaussian distribution? There are an infinite number of them,
with an infinite number of different means and standard deviations.
We’re ready to write down the general model and compute the plausibility
of each combination of μ and σ . To define the heights as normally
distributed with a mean μ and standard deviation σ, we write:</p>
<p>hi ~ Normal(μ, σ)</p>
<p>In many books you’ll see the same model written as hi ~ N(μ, σ),
which means the same thing. The symbol h refers to the list of heights,
and the subscript i means each individual element of this list. It is
conventional to use i because it stands for index. The index i takes on
row numbers, and so in this example can take any value from 1 to 352
(the number of heights in d2). As such, the model above is saying that
all the golem knows about each height measurement is defined by the same
normal distribution, with mean μ and standard deviation σ . Before long,
those little i’s are going to show up on the right-hand side of the
model definition, and you’ll be able to see why we must bother with
them. So don’t ignore the i, even if it seems like useless ornamentation
right now.</p>
<p>To complete the model, we’re going to need some priors. The
parameters to be estimated are both μ and σ, so we need a prior Pr(μ,
σ), the joint prior probability for all parameters. In most cases,
priors are specified independently for each parameter, which amounts to
as- suming Pr(μ, σ) = Pr(μ) Pr(σ). Then we can write:</p>
<p>hi ~ Normal(μ, σ)</p>
<p>μ ~ Normal(178, 20)</p>
<p>σ ~ Uniform(0, 50)</p>
<p>The labels on the right are not part of the model, but instead just
notes to help you keep track of the purpose of each line. The prior for
μ is a broad Gaussian prior, centered on 178 cm, with 95% of probability
between 178 ± 40 cm.</p>
<p>Why 178 cm? Your author is 178 cm tall. And the range from 138 cm to
218 cm encom- passes a huge range of plausible mean heights for human
populations. So domain-specific information has gone into this prior.
Everyone knows something about human height and can set a reasonable and
vague prior of this kind. But in many regression problems, as you’ll see
later, using prior information is more subtle, because parameters don’t
always have such clear physical meaning.</p>
<p>Whatever the prior, it’s a very good idea to plot your priors, so you
have a sense of the assumption they build into the model. In this
case:</p>
<p>R code 4.12</p>
<p>curve( dnorm( x , 178 , 20 ) , from=100 , to=250 )</p>
<p>Execute that code yourself, to see that the golem is assuming that
the average height (not each individual height) is almost certainly
between 140 cm and 220 cm. So this prior carries a little information,
but not a lot. The σ prior is a truly flat prior, a uniform one, that
functions just to constrain σ to have positive probability between zero
and 50 cm. View it with:</p>
<p>R code 4.13</p>
<p>curve( dunif( x , 0 , 50 ) , from=-10 , to=60 )</p>
<p>A standard deviation like σ must be positive, so bounding it at zero
makes sense. How should we pick the upper bound? In this case, a
standard deviation of 50 cm would imply that 95% of individual heights
lie within 100 cm of the average height. That’s a very large range.</p>
<p>All this talk is nice. But it’ll help to see what these priors imply
about the distribution of individual heights. The prior predictive
simulation is an essential part of your modeling. Once you’ve chosen
priors for h, μ, and σ, these imply a joint prior distribution of
individual heights. By simulating from this distribution, you can see
what your choices imply about observable height. This helps you diagnose
bad choices. Lots of conventional choices are indeed bad ones, and we’ll
be able to see this through prior predictive simulations.</p>
<p>Okay, so how to do this? You can quickly simulate heights by sampling
from the prior, like you sampled from the posterior back in 3. Remember,
every posterior is also potentially a prior for a subsequent analysis,
so you can process priors just like posteriors.</p>
<p>R code 4.14</p>
<p>sample_mu &lt;- rnorm( 1e4 , 178 , 20 )</p>
<p>sample_sigma &lt;- runif( 1e4 , 0 , 50 )</p>
<p>prior_h &lt;- rnorm( 1e4 , sample_mu , sample_sigma )</p>
<p>dens( prior_h )</p>
<p>mu ~ dnorm( 178 , 20 )</p>
<p>100</p>
<p>h ~ dnorm(mu,sigma)</p>
<p>0 73 178 283 height</p>
<p>sigma ~ dunif( 0 , 50 )</p>
<p>0</p>
<p>h ~ dnorm(mu,sigma)</p>
<p>mu ~ dnorm(178,100)</p>
<p>-128 0 178 484 height</p>
<p>Figure 4.3. Prior predictive simulation for the height model. Top
row: Prior distributions for μ and σ . Bottom left: The prior predictive
simulation for height, using the priors in the top row. Values at 3
standard deviations shown on horizontal axis. Bottom right: Prior
predictive simulation using μ ~ Normal(178, 100).</p>
<p>This density, as well as the individual densities for μ and σ, is
shown in Figure 4.3. It dis- plays a vaguely bell-shaped density with
thick tails. It is the expected distribution of heights, averaged over
the prior. Notice that the prior probability distribution of height is
not itself Gaussian. This is okay. The distribution you see is not an
empirical expectation, but rather the distribution of relative
plausibilities of different heights, before seeing the data.</p>
<p>Prior predictive simulation is very useful for assigning sensible
priors, because it can be quite hard to anticipate how priors influence
the observable variables. As an example, con- sider a much flatter and
less informative prior for μ, like μ ~ Normal(178, 100). Priors with
such large standard deviations are quite common in Bayesian models, but
they are hardly ever sensible. Let’s use simulation again to see the
implied heights:</p>
<p>The result is displayed in the lower right of Figure 4.3. Now the
model, before seeing the data, expects 4% of people, those left of the
dashed line, to have negative height. It also expects some giants. One
of the tallest people in recorded history, Robert Pershing Wadlow
(1918–1940) stood 272 cm tall. In our prior predictive simulation, 18%
of people (right of solid line) are taller than this.</p>
<p>Does this matter? In this case, we have so much data that the silly
prior is harmless. But that won’t always be the case. There are plenty
of inference problems for which the data alone are not sufficient, no
matter how numerous. Bayes lets us proceed in these cases. But only if
we use our scientific knowledge to construct sensible priors. Using
scientific knowledge to build priors is not cheating. The important
thing is that your prior not be based on the values in the data, but
only on what you know about the data before you see it.</p>
<p>Overthinking: Model definition to Bayes’theorem again. It can help to
see how the model definition on the previous page allows us to build up
the posterior distribution. The height model, with its priors for μ and
σ, defines this posterior distribution:</p>
<p>This looks monstrous, but it’s the same creature as before. There are
two new things that make it seem complicated. The first is that there is
more than one observation in h, so to get the joint likelihood across
all the data, we have to compute the probability for each hi and then
multiply all these likeli- hoods together. The product on the right-hand
side takes care of that. The second complication is the two priors, one
for μ and one for σ . But these just stack up. In the grid approximation
code in the section to follow, you’ll see the implications of this
definition in the R code. Everything will be calculated on the log
scale, so multiplication will become addition. But otherwise it’s just a
matter of executing Bayes’ theorem.</p>
<p>4.3.3. Grid approximation of the posterior distribution. Since this
is the first Gaussian model in the book, and indeed the first model with
more than one parameter, it’s worth quickly mapping out the posterior
distribution through brute force calculations. This isn’t the approach I
encourage in any other place, because it is laborious and
computationally ex- pensive. Indeed, it is usually so impractical as to
be essentially impossible. But as always, it is worth knowing what the
target actually looks like, before you start accepting approxima- tions
of it. A little later in this chapter, you’ll use quadratic
approximation to estimate the posterior distribution, and that’s the
approach you’ll use for several chapters more. Once you have the samples
you’ll produce in this subsection, you can compare them to the quadratic
approximation in the next.</p>
<p>Unfortunately, doing the calculations here requires some technical
tricks that add little, if any, conceptual insight. So I’m going to
present the code here without explanation. You can execute it and keep
going for now, but later return and follow the endnote for an
explanation of the algorithm.73 For now, here are the guts ofthe
golem:</p>
<p>You can inspect this posterior distribution, now residing in post,
using a variety of plotting commands. You can get a simple contour plot
with:</p>
<p>Or you can plot a simple heat map with:</p>
<p>The functions contour_xyz and image_xyz are both in the rethinking
package.</p>
<p>4.3.4. Sampling from the posterior. To study this posterior
distribution in more detail, again I’ll push the flexible approach of
sampling parameter values from it. This works just like it did in 3,
when you sampled values of p from the posterior distribution for the
globe tossing example. The only new trick is that since there are two
parameters, and we want to sample combinations of them, we first
randomly sample row numbers in post in proportion to the values in post.
Then we pull out the parameter values on those randomly sampled rows.
This code will do it:</p>
<p>You end up with 10,000 samples, with replacement, from the posterior
for the height data. Take a look at these samples:</p>
<p>I reproduce this plot in Figure 4.4. Note that the function col.alpha
is part of the rethink- ing R package. All it does is make colors
transparent, which helps the plot in Figure 4.4 more easily show
density, where samples overlap. Adjust the plot to your tastes by
playing around with cex (character expansion, the size of the points),
pch (plot character), and the 0.1 transparency value.</p>
<p>Figure 4.4. Samples from the posterior dis- tribution for the heights
data. The density of points is highest in the center, reflecting the
most plausible combinations of μ and σ . There are many more ways for
these parame- ter values to produce the data, conditional on the
model.</p>
<p>Now that you have these samples, you can describe the distribution of
confidence in each combination of μ and σ by summarizing the samples.
Think of them like data and describe them, just like in 3. For example,
to characterize the shapes ofthe marginal posterior densities of μ and
σ, all we need to do is:</p>
<p>R code 4.21</p>
<p>dens( sample.mu )</p>
<p>dens( sample.sigma )</p>
<p>The jargon “marginal” here means “averaging over the other
parameters.” Execute the above code and inspect the plots. These
densities are very close to being normal distributions. And this is
quite typical. As sample size increases, posterior densities approach
the normal distribution. If you look closely, though, you’ll notice that
the density for σ has a longer right-hand tail. I’ll exaggerate this
tendency a bit later, to show you that this condition is very common for
standard deviation parameters.</p>
<p>To summarize the widths of these densities with posterior
compatibility intervals:</p>
<p>R code 4.22</p>
<p>PI( sample.mu )</p>
<p>PI( sample.sigma )</p>
<p>Since these samples are just vectors of numbers, you can compute any
statistic from them that you could from ordinary data: mean, median, or
quantile, for example.</p>
<p>Overthinking: Sample size and the normality of σ,s posterior. Before
moving on to using quadratic approximation (quap) as shortcut to all of
this inference, it is worth repeating the analysis of the height data
above, but now with only a fraction of the original data. The reason to
do this is to demonstrate that, in principle, the posterior is not
always so Gaussian in shape. There’s no trouble with the mean, μ . For a
Gaussian likelihood and a Gaussian prior on μ, the posterior
distribution is always Gaussian as well, regardless of sample size. It
is the standard deviation σ that causes problems. So if you care about
σ—often people do not—you do need to be careful of abusing the quadratic
approximation.</p>
<p>The deep reasons for the posterior of σ tending to have a long
right-hand tail are complex. But a useful way to conceive of the problem
is that variances must be positive. As a result, there must be more
uncertainty about how big the variance (or standard deviation) is than
about how small it is.</p>
<p>For example, if the variance is estimated to be near zero, then you
know for sure that it can’t be much smaller. But it could be a lot
bigger.</p>
<p>Let’s quickly analyze only 20 of the heights from the height data to
reveal this issue. To sample 20 random heights from the original
list:</p>
<p>Now I’ll repeat all the code from the previous subsection, modified
to focus on the 20 heights in d3 rather than the original data. I’ll
compress all of the code together here.</p>
<p>After executing the code above, you’ll see another scatter plot of
the samples from the posterior den- sity, but this time you’ll notice a
distinctly longer tail at the top of the cloud of points. You should
also inspect the marginal posterior density for σ, averaging over μ,
produced with:</p>
<p>This code will also show a normal approximation with the same mean
and variance. Now you can see that the posterior for σ is not Gaussian,
but rather has a long tail towards higher values.</p>
<p>4.3.5. Finding the posterior distribution with quap. Now we leave
grid approximation be- hind and move on to one of the great engines of
applied statistics, the quadratic approxi- mation. Our interest in
quadratic approximation, recall, is as a handy way to quickly make
inferences about the shape of the posterior. The posterior’s peak will
lie at the maximum a posteriori estimate (MAP), and we can get a useful
image of the posterior’s shape by using the quadratic approximation of
the posterior distribution at this peak.</p>
<p>To build the quadratic approximation, we’ll use quap, a command in
the rethinking package. The quap function works by using the model
definition you were introduced to ear- lier in this chapter. Each line
in the definition has a corresponding definition in the form of R code.
The engine inside quap then uses these definitions to define the
posterior probability at each combination of parameter values. Then it
can climb the posterior distribution and find the peak, its MAP.
Finally, it estimates the quadratic curvature at the MAP to produce an
approximation of the posterior distribution. Remember: This procedure is
very similar to what many non-Bayesian procedures do, just without any
priors.</p>
<p>Let’s begin by repeating the code to load the data and select out the
adults:</p>
<p>R code 4.23</p>
<p>R code 4.24</p>
<p>R code 4.25</p>
<p>R code 4.26</p>
<p>R code 4.27</p>
<p>R code 4.28</p>
<p>R code 4.29</p>
<p>Now we’re ready to define the model, using R’s formula syntax. The
model definition in this case is just as before, but now we’ll repeat it
with each corresponding line of R code shown on the right-hand
margin:</p>
<p>hi ~ Normal(μ, σ)</p>
<p>μ ~ Normal(178, 20)</p>
<p>σ ~ Uniform(0, 50)</p>
<p>Now place the R code equivalents into an alist. Here’s an alist of
the formulas above:</p>
<p>Note the commas at the end of each line, except the last. These
commas separate each line of the model definition.</p>
<p>Fit the model to the data in the data frame d2 with:</p>
<p>After executing this code, you’ll have a fit model stored in the
symbol m4.1. Now take a look at the posterior distribution:</p>
<p>mean sd 5.5% 94.5% mu 154.61 0.41 153.95 155.27 sigma 7.73 0.29 7.27
8.20</p>
<p>These numbers provide Gaussian approximations for each parameter’s
marginal distribution. This means the plausibility of each value of μ,
after averaging over the plausibilities of each value of σ, is given by
a Gaussian distribution with mean 154.6 and standard deviation 0.4.</p>
<p>The 5.5% and 94.5% quantiles are percentile interval boundaries,
corresponding to an 89% compatibility interval. Why 89%? It’s just the
default. It displays a quite wide interval, so it shows a
high-probability range of parameter values. If you want another
interval, such as the conventional and mindless 95%, you can use
precis(m4.1,prob=0.95). But I don’t recommend 95% intervals, because
readers will have a hard time not viewing them as signif- icance tests.
89 is also a prime number, so if someone asks you to justify it, you can
stare at them meaningfully and incant, “Because it is prime.” That’s no
worse justification than the conventional justification for 95%.</p>
<p>I encourage you to compare these 89% boundaries to the compatibility
intervals from the grid approximation earlier. You’ll find that they are
almost identical. When the posterior is approximately Gaussian, then
this is what you should expect.</p>
<p>Overthinking: Start values for quap. quap estimates the posterior by
climbing it like a hill. To do this, it has to start climbing someplace,
at some combination of parameter values. Unless you tell it otherwise,
quap starts at random values sampled from the prior. But it’s also
possible to specify a starting value for any parameter in the model. In
the example in the previous section, that means the parameters μ and σ .
Here’s a good list of starting values in this case:</p>
<p>These start values are good guesses of the rough location of the MAP
values.</p>
<p>Note that the list of start values is a regular list, not an alist
like the formula list is. The two functions alist and list do the same
basic thing: allow you to make a collection of arbitrary R objects. They
differ in one important respect: list evaluates the code you embed
inside it, while alist does not. So when you define a list of formulas,
you should use alist, so the code isn’t ex- ecuted. But when you define
a list of start values for parameters, you should use list, so that code
like mean(d2) will be evaluated to a numeric value.</p>
<p>The priors we used before are very weak, both because they are nearly
flat and because there is so much data. So I’ll splice in a more
informative prior for μ, so you can see the effect. All I’m going to do
is change the standard deviation of the prior to 0.1, so it’s a very
narrow prior. I’ll also build the formula right into the call to quap
this time.</p>
<p>R code 4.30</p>
<p>R code 4.31</p>
<p>mean sd 5.5% 94.5% mu 177.86 0.10 177.70 178.02 sigma 24.52 0.93
23.03 26.00</p>
<p>Notice that the estimate for μ has hardly moved off the prior. The
prior was very concentrated around 178. So this is not surprising. But
also notice that the estimate for σ has changed quite a lot, even though
we didn’t change its prior at all. Once the golem is certain that the
mean is near 178—as the prior insists—then the golem has to estimate σ
conditional on that fact. This results in a different posterior for σ,
even though all we changed is prior information about the other
parameter.</p>
<p>4.3.6. Sampling from a quap. The above explains how to get a
quadratic approximation of the posterior, using quap. But how do you
then get samples from the quadratic approxi- mate posterior
distribution? The answer is rather simple, but non-obvious, and it
requires</p>
<p>R code 4.32</p>
<p>R code 4.33</p>
<p>recognizing that a quadratic approximation to a posterior
distribution with more than one parameter dimension—μ and σ each
contribute one dimension—is just a multi-dimensional Gaussian
distribution.</p>
<p>As a consequence, when R constructs a quadratic approximation, it
calculates not only standard deviations for all parameters, but also the
covariances among all pairs of param- eters. Just like a mean and
standard deviation (or its square, a variance) are sufficient to
describe a one-dimensional Gaussian distribution, a list of means and a
matrix of variances and covariances are sufficient to describe a
multi-dimensional Gaussian distribution. To see this matrix of variances
and covariances, for model m4.1, use:</p>
<p>mu sigma mu 0.1697395865 0.0002180593 sigma 0.0002180593
0.0849057933</p>
<p>The above is a variance-covariance matrix. It is the
multi-dimensional glue of a qua- dratic approximation, because it tells
us how each parameter relates to every other param- eter in the
posterior distribution. A variance-covariance matrix can be factored
into two elements: (1) a vector of variances for the parameters and (2)
a correlation matrix that tells us how changes in any parameter lead to
correlated changes in the others. This decomposi- tion is usually easier
to understand. So let’s do that now:</p>
<p>mu sigma</p>
<p>0.16973959 0.08490579</p>
<p>mu sigma mu 1.000000000 0.001816412 sigma 0.001816412 1.000000000</p>
<p>The two-element vector in the output is the list of variances. If you
take the square root of this vector, you get the standard deviations
that are shown in precis output. The two-by-two matrix in the output is
the correlation matrix. Each entry shows the correlation, bounded
between -1 and +1, for each pair of parameters. The 1’s indicate a
parameter’s correlation with itself. If these values were anything
except 1, we would be worried. The other entries are typically closer to
zero, and they are very close to zero in this example. This indicates
that learning μ tells us nothing about σ and likewise that learning σ
tells us nothing about μ . This is typical of simple Gaussian models of
this kind. But it is quite rare more generally, as you’ll see in later
chapters.</p>
<p>Okay, so how do we get samples from this multi-dimensional posterior?
Now instead of sampling single values from a simple Gaussian
distribution, we sample vectors of values from a multi-dimensional
Gaussian distribution. The rethinking package provides a con- venience
function to do exactly that:</p>
<p>mu sigma 1 155.0031 7.443893 2 154.0347 7.771255 3 154.9157 7.822178
4 154.4252 7.530331 5 154.5307 7.655490 6 155.1772 7.974603</p>
<p>You end up with a data frame, post, with 10,000 (1e4) rows and two
columns, one column for μ and one for σ . Each value is a sample from
the posterior, so the mean and standard deviation of each column will be
very close to the MAP values from before. You can confirm this by
summarizing the samples:</p>
<p>quap posterior: 10000 samples from m4 .1</p>
<p>mean sd 5 .5% 94 .5% histogram</p>
<p>mu 154 .61 0 .41 153 .95 155 .27 一一一▅▇▂一一</p>
<p>sigma 7 .72 0 .29 7 .26 8 .18 一一一▂▅▇▇▃一一一一</p>
<p>Compare these values to the output from precis(m4.1). And you can use
plot(post) to see how much they resemble the samples from the grid
approximation in Figure 4.4 (page 86). These samples also preserve the
covariance between μ and σ . This hardly matters right now, because μ
and σ don’t covary at all in this model. But once you add a predictor
variable to your model, covariance will matter a lot.</p>
<p>Overthinking: Under the hood with multivariate sampling. The function
extract .samples is for convenience. It is just running a simple
simulation of the sort you conducted near the end of 3. Here’s a peak at
the motor. The work is done by a multi-dimensional version of rnorm, mv
rnorm. The function rnorm simulates random Gaussian values, while mv
rnorm simulates random vectors of multivariate Gaussian values. Here’s
how to use it to do what extract .samples does:</p>
<p>You don’t usually need to use mvrnorm directly like this, but
sometimes you want to simulate multi- variate Gaussian outcomes. In that
case, you’ll need to access mv rnorm directly. And of course it’s always
good to know a little about how the machine operates. Later on, we’ll
work with posterior distributions that cannot be correctly approximated
this way.</p>
<p>R code 4.35</p>
<p>R code 4.36</p>
<h6 id="阅读日期-2025年12月19日-2025年12月19日-共-1-天">阅读日期：
2025年12月19日-2025年12月19日 共： 1 天</h6>
<script src="https://giscus.app/client.js"
        data-repo="hbaolong/hbaolong.github.io"
        data-repo-id="R_kgDOLetDQg"
        data-category="General"
        data-category-id="DIC_kwDOLetDQs4CfLEl"
        data-mapping="url"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="1"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="zh-CN"
        crossorigin="anonymous"
        async>
</script>
</body>
</html>
